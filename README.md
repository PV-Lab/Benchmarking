# Benchmarking
Benchmarking 

Project Name: (Tentative) Benchmarking

Project Leader: Harry Qiaohao Liang (hqliang@mit.edu)

Abstract: 
In the field of machine learning (ML) for materials optimization, active learning algorithms like Bayesian Optimization (BO) have been widely used to guide high- throughput autonomous experiments. However, very few studies have evaluated the efficiency of BO as a general optimization algorithm for a wide range of material systems. In this work, we apply a benchmarking framework to evaluate the performance of BO across five diverse experimental materials science domains. With acceleration and enhancement metrics defined for specific research objectives, we show that random forests (RF) paired with lower confidence bound (LCB) outclass other combinations of surrogate models and acquisition functions commonly used in BO. Our analysis of RFâ€™s performance and practicality as surrogate model has also yielded many useful insights on how materials scientists could select the most suitable BO algorithms to improve their optimization campaigns. 
