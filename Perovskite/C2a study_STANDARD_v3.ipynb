{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/liangqiaohao/Downloads/PV Lab/2020/March/Learning project/C2a\n"
     ]
    }
   ],
   "source": [
    "cd /Users/liangqiaohao/Downloads/PV Lab/2020/March/Learning project/C2a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import math\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import GPyOpt\n",
    "import GPy\n",
    "\n",
    "import os\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.tri as tri\n",
    "\n",
    "import ternary\n",
    "import pickle\n",
    "import datetime\n",
    "\n",
    "from collections import Counter\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "import pyDOE\n",
    "import random\n",
    "from scipy.stats import norm\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CsPbI</th>\n",
       "      <th>MAPbI</th>\n",
       "      <th>FAPbI</th>\n",
       "      <th>Instability index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>4.929210e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.75</td>\n",
       "      <td>9.480030e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.129496e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.290847e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.452513e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.88</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.048450e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.89</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.819935e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.91</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.822180e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.96</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.04</td>\n",
       "      <td>3.497070e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.478915e+05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>94 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    CsPbI  MAPbI  FAPbI  Instability index\n",
       "0    0.00   0.00   1.00       4.929210e+05\n",
       "1    0.00   0.25   0.75       9.480030e+05\n",
       "2    0.00   0.50   0.50       1.129496e+06\n",
       "3    0.00   0.75   0.25       1.290847e+06\n",
       "4    0.00   1.00   0.00       1.452513e+06\n",
       "..    ...    ...    ...                ...\n",
       "89   0.88   0.12   0.00       4.048450e+05\n",
       "90   0.89   0.11   0.00       4.819935e+05\n",
       "91   0.91   0.08   0.01       4.822180e+05\n",
       "92   0.96   0.00   0.04       3.497070e+05\n",
       "93   1.00   0.00   0.00       2.478915e+05\n",
       "\n",
       "[94 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(r'/Users/liangqiaohao/Downloads/PV Lab/2020/March/Learning project/C2a/C2a learning study.csv')\n",
    "X = dataset.iloc[:,0:5]\n",
    "\n",
    "X_run = X.groupby(['CsPbI', 'MAPbI', 'FAPbI'])['Instability index'].agg(lambda x: x.unique().mean())\n",
    "X_run = (X_run.to_frame()).reset_index()\n",
    "\n",
    "X_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_feature = X_run[['CsPbI', 'MAPbI', 'FAPbI']].values\n",
    "y = np.array(X_run['Instability index'].values)\n",
    "\n",
    "\n",
    "raw_mean = np.mean(y)\n",
    "raw_std = np.std(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = (y - raw_mean) / raw_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_initial = 2\n",
    "n_test = int(math.ceil(len(y) * 0.30))\n",
    "n_top = int(math.ceil(len(y) * 0.05))\n",
    "\n",
    "top_indices = list(X_run.sort_values('Instability index').head(n_top).index)\n",
    "\n",
    "# seed_list = [74,28,861,1526,2,151,19617,7,33,47302,66,552,671,25,1368,850,71,2148,2621,26, 6, 294, 9256, 851, 666]\n",
    "# 25\n",
    "\n",
    "seed_list = [5, 892, 91, 2345, 391, 77, 223, 8258, 16, 3, 84,7646,6207,4530,8206,6260,2342,3112,5187,2737,8979,4018,8223,1678,9481,2232,\n",
    "             461,7694,6091, 3189,2480,9106,1316,8500,2531,5329,9872,1388,8805,2726,119,3416,7747,3927,138,1488,232,4564,3492,9841]\n",
    "# 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[24, 33, 53, 14, 23]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EI(X, model, y_best):\n",
    "#     xi = 0.01\n",
    "    X = X.reshape([1,X_feature.shape[1]])\n",
    "    \n",
    "    mean, std = model.predict(X)[0][0][0], model.predict(X)[1][0][0]\n",
    "\n",
    "    z = (y_best - mean)/std\n",
    "    return (y_best - mean) * norm.cdf(z) + std * norm.pdf(z)\n",
    "\n",
    "def LCB(X, model):\n",
    "    X = X.reshape([1,X_feature.shape[1]])\n",
    "    \n",
    "    mean, std = model.predict(X)[0][0][0], model.predict(X)[1][0][0]\n",
    "    \n",
    "    return 1 * std - 1 * mean\n",
    "\n",
    "def MPI(X, model, y_best):\n",
    "    X = X.reshape([1,X_feature.shape[1]])\n",
    "    \n",
    "    mean, std = model.predict(X)[0][0][0], model.predict(X)[1][0][0]\n",
    "    \n",
    "    z = (y_best - mean)/std\n",
    "    return norm.cdf(z)\n",
    "\n",
    "def calc_MAE(test_index, SL_model):\n",
    "    SL_mae = 0\n",
    "    \n",
    "    for index in test_index:\n",
    "        X_test = X_feature[index]\n",
    "        y_test = y[index]\n",
    "        \n",
    "        X_test = X_test.reshape([1,X_feature.shape[1]])\n",
    "        \n",
    "        SL_mean, SL_std = SL_model.predict(X_test)[0][0][0], SL_model.predict(X_test)[1][0][0]\n",
    "        \n",
    "        sl_mae = (y_test - SL_mean)**2 \n",
    "        SL_mae += sl_mae       \n",
    "    \n",
    "    return SL_mae / len(test_index)\n",
    "\n",
    "def TopPercent(top_indices, index_):\n",
    "    how_many = 0\n",
    "    for i in index_:\n",
    "        if i in top_indices:\n",
    "            how_many += 1\n",
    "    return how_many\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARD_ = False\n",
    "\n",
    "Bias_kernel = GPy.kern.Bias(X_feature.shape[1], variance=1.)\n",
    "\n",
    "Matern52_kernel = GPy.kern.Matern52(X_feature.shape[1], variance=1., ARD=ARD_) + Bias_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing seed = 0\n",
      "Finished seed\n",
      "initializing seed = 1\n",
      "Finished seed\n",
      "initializing seed = 2\n",
      "Finished seed\n",
      "initializing seed = 3\n",
      "Finished seed\n",
      "initializing seed = 4\n",
      "Finished seed\n",
      "initializing seed = 5\n",
      "Finished seed\n",
      "initializing seed = 6\n",
      "Finished seed\n",
      "initializing seed = 7\n",
      "Finished seed\n",
      "initializing seed = 8\n",
      "Finished seed\n",
      "initializing seed = 9\n",
      "Finished seed\n",
      "initializing seed = 10\n",
      "Finished seed\n",
      "initializing seed = 11\n",
      "Finished seed\n",
      "initializing seed = 12\n",
      "Finished seed\n",
      "initializing seed = 13\n",
      "Finished seed\n",
      "initializing seed = 14\n",
      "Finished seed\n",
      "initializing seed = 15\n",
      "Finished seed\n",
      "initializing seed = 16\n",
      "Finished seed\n",
      "initializing seed = 17\n",
      "Finished seed\n",
      "initializing seed = 18\n",
      "Finished seed\n",
      "initializing seed = 19\n",
      "Finished seed\n",
      "initializing seed = 20\n",
      "Finished seed\n",
      "initializing seed = 21\n",
      "Finished seed\n",
      "initializing seed = 22\n",
      "Finished seed\n",
      "initializing seed = 23\n",
      "Finished seed\n",
      "initializing seed = 24\n",
      "Finished seed\n",
      "initializing seed = 25\n",
      "Finished seed\n",
      "initializing seed = 26\n",
      "Finished seed\n",
      "initializing seed = 27\n",
      "Finished seed\n",
      "initializing seed = 28\n",
      "Finished seed\n",
      "initializing seed = 29\n",
      "Finished seed\n",
      "initializing seed = 30\n",
      "Finished seed\n",
      "initializing seed = 31\n",
      "Finished seed\n",
      "initializing seed = 32\n",
      "Finished seed\n",
      "initializing seed = 33\n",
      "Finished seed\n",
      "initializing seed = 34\n",
      "Finished seed\n",
      "initializing seed = 35\n",
      "Finished seed\n",
      "initializing seed = 36\n",
      "Finished seed\n",
      "initializing seed = 37\n",
      "Finished seed\n",
      "initializing seed = 38\n",
      "Finished seed\n",
      "initializing seed = 39\n",
      "Finished seed\n",
      "initializing seed = 40\n",
      "Finished seed\n",
      "initializing seed = 41\n",
      "Finished seed\n",
      "initializing seed = 42\n",
      "Finished seed\n",
      "initializing seed = 43\n",
      "Finished seed\n",
      "initializing seed = 44\n",
      "Finished seed\n",
      "initializing seed = 45\n",
      "Finished seed\n",
      "initializing seed = 46\n",
      "Finished seed\n",
      "initializing seed = 47\n",
      "Finished seed\n",
      "initializing seed = 48\n",
      "Finished seed\n",
      "initializing seed = 49\n",
      "Finished seed\n"
     ]
    }
   ],
   "source": [
    "index_collection = []\n",
    "X_collection = []\n",
    "y_collection = []\n",
    "TopPercent_collection = []\n",
    "MAE_collection = []\n",
    "\n",
    "\n",
    "for s in seed_list:\n",
    "    print('initializing seed = ' +str(seed_list.index(s)))\n",
    "    random.seed(s)\n",
    "    \n",
    "    indices = list(np.arange(len(X_run)))\n",
    "    \n",
    "    \n",
    "    X_top = []\n",
    "    y_top = []\n",
    "    for i in top_indices:\n",
    "        X_top.append(X_feature[i])\n",
    "        y_top.append(y[i])\n",
    "        \n",
    "    index_learn = indices.copy()\n",
    "    \n",
    "    index_ = random.sample(index_learn, n_initial)\n",
    "    X_ = []\n",
    "    y_ = []\n",
    "    TopPercent_ = []\n",
    "    MAE_ = []\n",
    "    for i in index_:\n",
    "        X_.append(X_feature[i])\n",
    "        y_.append(y[i])\n",
    "        index_learn.remove(i)\n",
    "           \n",
    "\n",
    "    \n",
    "    for i in np.arange(len(index_learn)):\n",
    "        \n",
    "        y_best = np.min(y_)\n",
    "        \n",
    "        GP_learn = GPy.models.GPRegression(X = np.array(X_), \n",
    "                                           Y = np.array([[i] for i in y_]), \n",
    "                                           kernel= Matern52_kernel,\n",
    "                                           noise_var = 0.01\n",
    "                                          )\n",
    "\n",
    "        GP_learn.optimize_restarts(num_restarts=10,\n",
    "                                   parallel = True,\n",
    "                                   robust = True,\n",
    "                                   optimizer = 'bfgs',\n",
    "                                   max_iters=100,\n",
    "                                   verbose = False)\n",
    "        \n",
    "        mae_ = calc_MAE(index_learn, GP_learn)\n",
    "        MAE_.append(mae_)\n",
    "        TopPercent_.append(TopPercent(top_indices, index_))\n",
    "        \n",
    "        next_index = None\n",
    "        max_ac = -10**10\n",
    "        for j in index_learn:\n",
    "            X_j = X_feature[j]\n",
    "            y_j = y[j]\n",
    "            \n",
    "            ac_value = EI(X_j, GP_learn, y_best)\n",
    "            \n",
    "            if max_ac <= ac_value:\n",
    "                max_ac = ac_value\n",
    "                next_index = j\n",
    "                \n",
    "        X_.append(X_feature[next_index])\n",
    "        y_.append(y[next_index])\n",
    "        index_learn.remove(next_index)\n",
    "        index_.append(next_index)        \n",
    "\n",
    "\n",
    "    index_collection.append(index_)\n",
    "    X_collection.append(X_)\n",
    "    y_collection.append(y_)\n",
    "    TopPercent_collection.append(TopPercent_)\n",
    "    MAE_collection.append(MAE_)\n",
    "    \n",
    "    print('Finished seed')\n",
    "    \n",
    "    \n",
    "master = np.array([index_collection, X_collection, y_collection, TopPercent_collection, MAE_collection])\n",
    "np.save('STANDARD_GP_EI_master', master)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing seed = 0\n",
      "Finished seed\n",
      "initializing seed = 1\n",
      "Finished seed\n",
      "initializing seed = 2\n",
      "Finished seed\n",
      "initializing seed = 3\n",
      "Finished seed\n",
      "initializing seed = 4\n",
      "Finished seed\n",
      "initializing seed = 5\n",
      "Finished seed\n",
      "initializing seed = 6\n",
      "Finished seed\n",
      "initializing seed = 7\n",
      "Finished seed\n",
      "initializing seed = 8\n",
      "Finished seed\n",
      "initializing seed = 9\n",
      "Finished seed\n",
      "initializing seed = 10\n",
      "Finished seed\n",
      "initializing seed = 11\n",
      "Finished seed\n",
      "initializing seed = 12\n",
      "Finished seed\n",
      "initializing seed = 13\n",
      "Finished seed\n",
      "initializing seed = 14\n",
      "Finished seed\n",
      "initializing seed = 15\n",
      "Finished seed\n",
      "initializing seed = 16\n",
      "Finished seed\n",
      "initializing seed = 17\n",
      "Finished seed\n",
      "initializing seed = 18\n",
      "Finished seed\n",
      "initializing seed = 19\n",
      "Finished seed\n",
      "initializing seed = 20\n",
      "Finished seed\n",
      "initializing seed = 21\n",
      "Finished seed\n",
      "initializing seed = 22\n",
      "Finished seed\n",
      "initializing seed = 23\n",
      "Finished seed\n",
      "initializing seed = 24\n",
      "Finished seed\n",
      "initializing seed = 25\n",
      "Finished seed\n",
      "initializing seed = 26\n",
      "Finished seed\n",
      "initializing seed = 27\n",
      "Finished seed\n",
      "initializing seed = 28\n",
      "Finished seed\n",
      "initializing seed = 29\n",
      "Finished seed\n",
      "initializing seed = 30\n",
      "Finished seed\n",
      "initializing seed = 31\n",
      "Finished seed\n",
      "initializing seed = 32\n",
      "Finished seed\n",
      "initializing seed = 33\n",
      "Finished seed\n",
      "initializing seed = 34\n",
      "Finished seed\n",
      "initializing seed = 35\n",
      "Finished seed\n",
      "initializing seed = 36\n",
      "Finished seed\n",
      "initializing seed = 37\n",
      "Finished seed\n",
      "initializing seed = 38\n",
      "Finished seed\n",
      "initializing seed = 39\n",
      "Finished seed\n",
      "initializing seed = 40\n",
      "Finished seed\n",
      "initializing seed = 41\n",
      "Finished seed\n",
      "initializing seed = 42\n",
      "Finished seed\n",
      "initializing seed = 43\n",
      "Finished seed\n",
      "initializing seed = 44\n",
      "Finished seed\n",
      "initializing seed = 45\n",
      "Finished seed\n",
      "initializing seed = 46\n",
      "Finished seed\n",
      "initializing seed = 47\n",
      "Finished seed\n",
      "initializing seed = 48\n",
      "Finished seed\n",
      "initializing seed = 49\n",
      "Finished seed\n"
     ]
    }
   ],
   "source": [
    "index_collection = []\n",
    "X_collection = []\n",
    "y_collection = []\n",
    "TopPercent_collection = []\n",
    "MAE_collection = []\n",
    "\n",
    "\n",
    "for s in seed_list:\n",
    "    print('initializing seed = ' +str(seed_list.index(s)))\n",
    "    random.seed(s)\n",
    "    \n",
    "    indices = list(np.arange(len(X_run)))\n",
    "    \n",
    "    \n",
    "    X_top = []\n",
    "    y_top = []\n",
    "    for i in top_indices:\n",
    "        X_top.append(X_feature[i])\n",
    "        y_top.append(y[i])\n",
    "        \n",
    "    index_learn = indices.copy()\n",
    "    \n",
    "    index_ = random.sample(index_learn, n_initial)\n",
    "    X_ = []\n",
    "    y_ = []\n",
    "    TopPercent_ = []\n",
    "    MAE_ = []\n",
    "    for i in index_:\n",
    "        X_.append(X_feature[i])\n",
    "        y_.append(y[i])\n",
    "        index_learn.remove(i)\n",
    "           \n",
    "\n",
    "    \n",
    "    for i in np.arange(len(index_learn)):\n",
    "        \n",
    "        y_best = np.min(y_)\n",
    "        \n",
    "        GP_learn = GPy.models.GPRegression(X = np.array(X_), \n",
    "                                           Y = np.array([[i] for i in y_]), \n",
    "                                           kernel= Matern52_kernel,\n",
    "                                           noise_var = 0.01\n",
    "                                          )\n",
    "\n",
    "        GP_learn.optimize_restarts(num_restarts=10,\n",
    "                                   parallel = True,\n",
    "                                   robust = True,\n",
    "                                   optimizer = 'bfgs',\n",
    "                                   max_iters=100,\n",
    "                                   verbose = False)\n",
    "        \n",
    "        mae_ = calc_MAE(index_learn, GP_learn)\n",
    "        MAE_.append(mae_)\n",
    "        TopPercent_.append(TopPercent(top_indices, index_))\n",
    "        \n",
    "        next_index = None\n",
    "        max_ac = -10**10\n",
    "        for j in index_learn:\n",
    "            X_j = X_feature[j]\n",
    "            y_j = y[j]\n",
    "            \n",
    "            ac_value = LCB(X_j, GP_learn)\n",
    "            \n",
    "            if max_ac <= ac_value:\n",
    "                max_ac = ac_value\n",
    "                next_index = j\n",
    "                \n",
    "        X_.append(X_feature[next_index])\n",
    "        y_.append(y[next_index])\n",
    "        index_learn.remove(next_index)\n",
    "        index_.append(next_index)        \n",
    "\n",
    "\n",
    "    index_collection.append(index_)\n",
    "    X_collection.append(X_)\n",
    "    y_collection.append(y_)\n",
    "    TopPercent_collection.append(TopPercent_)\n",
    "    MAE_collection.append(MAE_)\n",
    "    \n",
    "    print('Finished seed')\n",
    "    \n",
    "    \n",
    "master = np.array([index_collection, X_collection, y_collection, TopPercent_collection, MAE_collection])\n",
    "np.save('STANDARD_GP_LCB11_master', master)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_est = 50\n",
    "# for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EI(X, RF_model, y_best):\n",
    "\n",
    "    \n",
    "    tree_predictions = []\n",
    "    for j in np.arange(n_est):\n",
    "        tree_predictions.append((RF_model.estimators_[j].predict(np.array([X]))).tolist())\n",
    "    mean = np.mean(np.array(tree_predictions), axis=0)[0]\n",
    "\n",
    "    \n",
    "    std = np.std(np.array(tree_predictions), axis=0)[0]\n",
    "    \n",
    "    z = (y_best - mean)/std\n",
    "    return (y_best - mean) * norm.cdf(z) + std * norm.pdf(z)\n",
    "\n",
    "def LCB(X, RF_model):\n",
    "    \n",
    "    tree_predictions = []\n",
    "    for j in np.arange(n_est):\n",
    "        tree_predictions.append((RF_model.estimators_[j].predict(np.array([X]))).tolist())\n",
    "    mean = np.mean(np.array(tree_predictions), axis=0)[0]\n",
    "\n",
    "    \n",
    "    std = np.std(np.array(tree_predictions), axis=0)[0]\n",
    "\n",
    "    \n",
    "    return 0 * std - 1 * mean\n",
    "\n",
    "def MPI(X, RF_model, y_best):\n",
    "    \n",
    "    tree_predictions = []\n",
    "    for j in np.arange(n_est):\n",
    "        tree_predictions.append((RF_model.estimators_[j].predict(np.array([X]))).tolist())\n",
    "    mean = np.mean(np.array(tree_predictions), axis=0)[0]\n",
    "    \n",
    "    std = np.std(np.array(tree_predictions), axis=0)[0]\n",
    "\n",
    "    \n",
    "    z = (y_best - mean)/std\n",
    "    return norm.cdf(z)\n",
    "\n",
    "\n",
    "def calc_MAE(test_index, RF_model):\n",
    "    RF_mae = 0\n",
    "    for index in test_index:\n",
    "        X_test = X_feature[index]\n",
    "        y_test = y[index]\n",
    "\n",
    "        tree_predictions = []\n",
    "        for j in np.arange(n_est):\n",
    "            tree_predictions.append((RF_model.estimators_[j].predict(np.array([X_test]))).tolist())\n",
    "        mean = np.mean(np.array(tree_predictions), axis=0)[0]\n",
    "\n",
    "        std = np.std(np.array(tree_predictions), axis=0)[0]\n",
    "\n",
    "        mae_ = (y_test - mean)**2 \n",
    "        RF_mae += mae_\n",
    "        \n",
    "    return RF_mae / len(test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing seed = 0\n",
      "Finished seed\n",
      "initializing seed = 1\n",
      "Finished seed\n",
      "initializing seed = 2\n",
      "Finished seed\n",
      "initializing seed = 3\n",
      "Finished seed\n",
      "initializing seed = 4\n",
      "Finished seed\n",
      "initializing seed = 5\n",
      "Finished seed\n",
      "initializing seed = 6\n",
      "Finished seed\n",
      "initializing seed = 7\n",
      "Finished seed\n",
      "initializing seed = 8\n",
      "Finished seed\n",
      "initializing seed = 9\n",
      "Finished seed\n",
      "initializing seed = 10\n",
      "Finished seed\n",
      "initializing seed = 11\n",
      "Finished seed\n",
      "initializing seed = 12\n",
      "Finished seed\n",
      "initializing seed = 13\n",
      "Finished seed\n",
      "initializing seed = 14\n",
      "Finished seed\n",
      "initializing seed = 15\n",
      "Finished seed\n",
      "initializing seed = 16\n",
      "Finished seed\n",
      "initializing seed = 17\n",
      "Finished seed\n",
      "initializing seed = 18\n",
      "Finished seed\n",
      "initializing seed = 19\n",
      "Finished seed\n",
      "initializing seed = 20\n",
      "Finished seed\n",
      "initializing seed = 21\n",
      "Finished seed\n",
      "initializing seed = 22\n",
      "Finished seed\n",
      "initializing seed = 23\n",
      "Finished seed\n",
      "initializing seed = 24\n",
      "Finished seed\n",
      "initializing seed = 25\n",
      "Finished seed\n",
      "initializing seed = 26\n",
      "Finished seed\n",
      "initializing seed = 27\n",
      "Finished seed\n",
      "initializing seed = 28\n",
      "Finished seed\n",
      "initializing seed = 29\n",
      "Finished seed\n",
      "initializing seed = 30\n",
      "Finished seed\n",
      "initializing seed = 31\n",
      "Finished seed\n",
      "initializing seed = 32\n",
      "Finished seed\n",
      "initializing seed = 33\n",
      "Finished seed\n",
      "initializing seed = 34\n",
      "Finished seed\n",
      "initializing seed = 35\n",
      "Finished seed\n",
      "initializing seed = 36\n",
      "Finished seed\n",
      "initializing seed = 37\n",
      "Finished seed\n",
      "initializing seed = 38\n",
      "Finished seed\n",
      "initializing seed = 39\n",
      "Finished seed\n",
      "initializing seed = 40\n",
      "Finished seed\n",
      "initializing seed = 41\n",
      "Finished seed\n",
      "initializing seed = 42\n",
      "Finished seed\n",
      "initializing seed = 43\n",
      "Finished seed\n",
      "initializing seed = 44\n",
      "Finished seed\n",
      "initializing seed = 45\n",
      "Finished seed\n",
      "initializing seed = 46\n",
      "Finished seed\n",
      "initializing seed = 47\n",
      "Finished seed\n",
      "initializing seed = 48\n",
      "Finished seed\n",
      "initializing seed = 49\n",
      "Finished seed\n"
     ]
    }
   ],
   "source": [
    "index_collection = []\n",
    "X_collection = []\n",
    "y_collection = []\n",
    "TopPercent_collection = []\n",
    "MAE_collection = []\n",
    "\n",
    "\n",
    "for s in seed_list:\n",
    "    print('initializing seed = ' +str(seed_list.index(s)))\n",
    "    random.seed(s)\n",
    "    \n",
    "    indices = list(np.arange(len(X_run)))\n",
    "    \n",
    "    \n",
    "    X_top = []\n",
    "    y_top = []\n",
    "    for i in top_indices:\n",
    "        X_top.append(X_feature[i])\n",
    "        y_top.append(y[i])\n",
    "        \n",
    "    index_learn = indices.copy()\n",
    "    \n",
    "    index_ = random.sample(index_learn, n_initial)\n",
    "    X_ = []\n",
    "    y_ = []\n",
    "    TopPercent_ = []\n",
    "    MAE_ = []\n",
    "    for i in index_:\n",
    "        X_.append(X_feature[i])\n",
    "        y_.append(y[i])\n",
    "        index_learn.remove(i)\n",
    "                \n",
    "    for i in np.arange(len(index_learn)):\n",
    "        \n",
    "        y_best = np.min(y_)\n",
    "        \n",
    "        RF_model = RandomForestRegressor(n_estimators= n_est)\n",
    "        RF_model.fit(X_, y_)\n",
    "        \n",
    "        mae_ = calc_MAE(index_learn, RF_model)\n",
    "        MAE_.append(mae_)\n",
    "        TopPercent_.append(TopPercent(top_indices, index_))\n",
    "        \n",
    "        next_index = None\n",
    "        max_ac = -10**10\n",
    "        for j in index_learn:\n",
    "            X_j = X_feature[j]\n",
    "            y_j = y[j]\n",
    "            \n",
    "            ac_value = EI(X_j, RF_model, y_best)\n",
    "            \n",
    "            if max_ac <= ac_value:\n",
    "                max_ac = ac_value\n",
    "                next_index = j\n",
    "                \n",
    "        X_.append(X_feature[next_index])\n",
    "        y_.append(y[next_index])\n",
    "        index_learn.remove(next_index)\n",
    "        index_.append(next_index)\n",
    "\n",
    "\n",
    "    index_collection.append(index_)\n",
    "    X_collection.append(X_)\n",
    "    y_collection.append(y_)\n",
    "    TopPercent_collection.append(TopPercent_)\n",
    "    MAE_collection.append(MAE_)\n",
    "    \n",
    "    print('Finished seed')\n",
    "    \n",
    "    \n",
    "master = np.array([index_collection, X_collection, y_collection, TopPercent_collection, MAE_collection])\n",
    "np.save('STANDARD_RF_EI_master', master)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing seed = 0\n",
      "Finished seed\n",
      "initializing seed = 1\n",
      "Finished seed\n",
      "initializing seed = 2\n",
      "Finished seed\n",
      "initializing seed = 3\n",
      "Finished seed\n",
      "initializing seed = 4\n",
      "Finished seed\n",
      "initializing seed = 5\n",
      "Finished seed\n",
      "initializing seed = 6\n",
      "Finished seed\n",
      "initializing seed = 7\n",
      "Finished seed\n",
      "initializing seed = 8\n",
      "Finished seed\n",
      "initializing seed = 9\n",
      "Finished seed\n",
      "initializing seed = 10\n",
      "Finished seed\n",
      "initializing seed = 11\n",
      "Finished seed\n",
      "initializing seed = 12\n",
      "Finished seed\n",
      "initializing seed = 13\n",
      "Finished seed\n",
      "initializing seed = 14\n",
      "Finished seed\n",
      "initializing seed = 15\n",
      "Finished seed\n",
      "initializing seed = 16\n",
      "Finished seed\n",
      "initializing seed = 17\n",
      "Finished seed\n",
      "initializing seed = 18\n",
      "Finished seed\n",
      "initializing seed = 19\n",
      "Finished seed\n",
      "initializing seed = 20\n",
      "Finished seed\n",
      "initializing seed = 21\n",
      "Finished seed\n",
      "initializing seed = 22\n",
      "Finished seed\n",
      "initializing seed = 23\n",
      "Finished seed\n",
      "initializing seed = 24\n",
      "Finished seed\n",
      "initializing seed = 25\n",
      "Finished seed\n",
      "initializing seed = 26\n",
      "Finished seed\n",
      "initializing seed = 27\n",
      "Finished seed\n",
      "initializing seed = 28\n",
      "Finished seed\n",
      "initializing seed = 29\n",
      "Finished seed\n",
      "initializing seed = 30\n",
      "Finished seed\n",
      "initializing seed = 31\n",
      "Finished seed\n",
      "initializing seed = 32\n",
      "Finished seed\n",
      "initializing seed = 33\n",
      "Finished seed\n",
      "initializing seed = 34\n",
      "Finished seed\n",
      "initializing seed = 35\n",
      "Finished seed\n",
      "initializing seed = 36\n",
      "Finished seed\n",
      "initializing seed = 37\n",
      "Finished seed\n",
      "initializing seed = 38\n",
      "Finished seed\n",
      "initializing seed = 39\n",
      "Finished seed\n",
      "initializing seed = 40\n",
      "Finished seed\n",
      "initializing seed = 41\n",
      "Finished seed\n",
      "initializing seed = 42\n",
      "Finished seed\n",
      "initializing seed = 43\n",
      "Finished seed\n",
      "initializing seed = 44\n",
      "Finished seed\n",
      "initializing seed = 45\n",
      "Finished seed\n",
      "initializing seed = 46\n",
      "Finished seed\n",
      "initializing seed = 47\n",
      "Finished seed\n",
      "initializing seed = 48\n",
      "Finished seed\n",
      "initializing seed = 49\n",
      "Finished seed\n"
     ]
    }
   ],
   "source": [
    "index_collection = []\n",
    "X_collection = []\n",
    "y_collection = []\n",
    "TopPercent_collection = []\n",
    "MAE_collection = []\n",
    "\n",
    "\n",
    "for s in seed_list:\n",
    "    print('initializing seed = ' +str(seed_list.index(s)))\n",
    "    random.seed(s)\n",
    "    \n",
    "    indices = list(np.arange(len(X_run)))\n",
    "    \n",
    "    \n",
    "    X_top = []\n",
    "    y_top = []\n",
    "    for i in top_indices:\n",
    "        X_top.append(X_feature[i])\n",
    "        y_top.append(y[i])\n",
    "        \n",
    "    index_learn = indices.copy()\n",
    "    \n",
    "    index_ = random.sample(index_learn, n_initial)\n",
    "    X_ = []\n",
    "    y_ = []\n",
    "    TopPercent_ = []\n",
    "    MAE_ = []\n",
    "    for i in index_:\n",
    "        X_.append(X_feature[i])\n",
    "        y_.append(y[i])\n",
    "        index_learn.remove(i)\n",
    "                \n",
    "    for i in np.arange(len(index_learn)):\n",
    "        \n",
    "        y_best = np.min(y_)\n",
    "        \n",
    "        RF_model = RandomForestRegressor(n_estimators= n_est)\n",
    "        RF_model.fit(X_, y_)\n",
    "        \n",
    "        mae_ = calc_MAE(index_learn, RF_model)\n",
    "        MAE_.append(mae_)\n",
    "        TopPercent_.append(TopPercent(top_indices, index_))\n",
    "        \n",
    "        next_index = None\n",
    "        max_ac = -10**10\n",
    "        for j in index_learn:\n",
    "            X_j = X_feature[j]\n",
    "            y_j = y[j]\n",
    "            \n",
    "            ac_value = LCB(X_j, RF_model)\n",
    "            \n",
    "            if max_ac <= ac_value:\n",
    "                max_ac = ac_value\n",
    "                next_index = j\n",
    "                \n",
    "        X_.append(X_feature[next_index])\n",
    "        y_.append(y[next_index])\n",
    "        index_learn.remove(next_index)\n",
    "        index_.append(next_index)\n",
    "\n",
    "\n",
    "    index_collection.append(index_)\n",
    "    X_collection.append(X_)\n",
    "    y_collection.append(y_)\n",
    "    TopPercent_collection.append(TopPercent_)\n",
    "    MAE_collection.append(MAE_)\n",
    "    \n",
    "    print('Finished seed')\n",
    "    \n",
    "    \n",
    "master = np.array([index_collection, X_collection, y_collection, TopPercent_collection, MAE_collection])\n",
    "np.save('STANDARD_RF_LCB01_master', master)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as before, load the data\n",
    "dtrain = xgb.DMatrix(data = X, label=y)\n",
    "# dtrain = xgb.DMatrix(np.array(X_feature), label=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "# https://xgboost.readthedocs.io/en/latest/parameter.html\n",
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_cv(max_depth, gamma, n_estimators ,eta):\n",
    "    params = {'max_depth': int(max_depth),\n",
    "              'gamma': gamma,\n",
    "              'n_estimators': int(n_estimators),\n",
    "              'eta': eta,\n",
    "              'subsample':0.6,\n",
    "              'eval_metric': 'rmse'}\n",
    "    cv_result = xgb.cv(params, dtrain, num_boost_round=50, nfold=5)\n",
    "#     we want to minimize error, so adding negative sign here because the BO later maximizes things\n",
    "    return -1.0 * cv_result['test-rmse-mean'].iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set param range, I find these 4 most important\n",
    "param_range = {'max_depth': (3, 10), \n",
    "               'gamma': (0, 1),\n",
    "               'n_estimators':(30,200),\n",
    "               'eta':(0,1)\n",
    "              }\n",
    "\n",
    "xgb_bo = BayesianOptimization(xgb_cv, param_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |    eta    |   gamma   | max_depth | n_esti... |\n",
      "-------------------------------------------------------------------------\n",
      "[09:21:21] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:480: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:21:21] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:480: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:21:21] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:480: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:21:21] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:480: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:21:21] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:480: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m nan     \u001b[0m | \u001b[0m 0.4479  \u001b[0m | \u001b[0m 0.7338  \u001b[0m | \u001b[0m 3.846   \u001b[0m | \u001b[0m 100.1   \u001b[0m |\n",
      "[09:21:21] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:480: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:21:21] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:480: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:21:21] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:480: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:21:21] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:480: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:21:21] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:480: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m nan     \u001b[0m | \u001b[0m 0.9662  \u001b[0m | \u001b[0m 0.02265 \u001b[0m | \u001b[0m 8.339   \u001b[0m | \u001b[0m 36.07   \u001b[0m |\n",
      "[09:21:21] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:480: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:21:21] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:480: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:21:21] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:480: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:21:21] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:480: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:21:21] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:480: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m nan     \u001b[0m | \u001b[0m 0.4795  \u001b[0m | \u001b[0m 0.684   \u001b[0m | \u001b[0m 5.202   \u001b[0m | \u001b[0m 30.76   \u001b[0m |\n",
      "[09:21:21] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:480: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:21:21] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:480: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:21:21] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:480: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:21:21] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:480: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:21:21] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:480: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m nan     \u001b[0m | \u001b[0m 0.4255  \u001b[0m | \u001b[0m 0.4762  \u001b[0m | \u001b[0m 8.023   \u001b[0m | \u001b[0m 141.8   \u001b[0m |\n",
      "[09:21:22] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:480: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:21:22] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:480: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:21:22] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:480: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:21:22] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:480: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:21:22] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:480: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m nan     \u001b[0m | \u001b[0m 0.5275  \u001b[0m | \u001b[0m 0.7409  \u001b[0m | \u001b[0m 9.496   \u001b[0m | \u001b[0m 113.8   \u001b[0m |\n",
      "[09:21:22] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:480: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:21:22] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:480: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:21:22] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:480: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:21:22] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:480: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:21:22] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:480: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m nan     \u001b[0m | \u001b[0m 0.2054  \u001b[0m | \u001b[0m 0.9301  \u001b[0m | \u001b[0m 8.722   \u001b[0m | \u001b[0m 132.7   \u001b[0m |\n",
      "[09:21:22] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:480: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:21:22] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:480: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:21:22] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:480: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:21:22] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:480: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:21:22] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:480: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m nan     \u001b[0m | \u001b[0m 0.3469  \u001b[0m | \u001b[0m 0.6701  \u001b[0m | \u001b[0m 9.129   \u001b[0m | \u001b[0m 183.5   \u001b[0m |\n",
      "[09:21:22] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:480: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:21:22] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:480: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:21:22] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:480: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:21:22] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:480: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:21:22] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:480: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m nan     \u001b[0m | \u001b[0m 0.9868  \u001b[0m | \u001b[0m 0.3035  \u001b[0m | \u001b[0m 5.79    \u001b[0m | \u001b[0m 147.6   \u001b[0m |\n",
      "[09:21:22] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:480: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:21:22] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:480: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:21:22] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:480: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:21:22] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:480: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:21:22] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:480: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m nan     \u001b[0m | \u001b[0m 0.7977  \u001b[0m | \u001b[0m 0.4911  \u001b[0m | \u001b[0m 8.2     \u001b[0m | \u001b[0m 122.8   \u001b[0m |\n",
      "[09:21:22] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:480: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:21:22] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:480: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:21:22] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:480: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:21:22] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:480: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:21:22] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:480: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 10      \u001b[0m | \u001b[0m nan     \u001b[0m | \u001b[0m 0.3588  \u001b[0m | \u001b[0m 0.5586  \u001b[0m | \u001b[0m 4.238   \u001b[0m | \u001b[0m 128.6   \u001b[0m |\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/bayes_opt/bayesian_optimization.py\u001b[0m in \u001b[0;36mmaximize\u001b[0;34m(self, init_points, n_iter, acq, kappa, kappa_decay, kappa_decay_delay, xi, **gp_params)\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m                 \u001b[0mx_probe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_queue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/bayes_opt/bayesian_optimization.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Queue is empty, no more objects to retrieve.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_queue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mStopIteration\u001b[0m: Queue is empty, no more objects to retrieve.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-77-1643e84b5739>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# initial sampling = 10, BO for 50 iterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mxgb_bo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_points\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ei'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/bayes_opt/bayesian_optimization.py\u001b[0m in \u001b[0;36mmaximize\u001b[0;34m(self, init_points, n_iter, acq, kappa, kappa_decay, kappa_decay_delay, xi, **gp_params)\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m                 \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m                 \u001b[0mx_probe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuggest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m                 \u001b[0miteration\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/bayes_opt/bayesian_optimization.py\u001b[0m in \u001b[0;36msuggest\u001b[0;34m(self, utility_function)\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_warnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# Finding argmax of the acquisition function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/gaussian_process/_gpr.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_vector_input\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m             X, y = self._validate_data(X, y, multi_output=True, y_numeric=True,\n\u001b[0;32m--> 190\u001b[0;31m                                        ensure_2d=True, dtype=\"numeric\")\n\u001b[0m\u001b[1;32m    191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             X, y = self._validate_data(X, y, multi_output=True, y_numeric=True,\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    430\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m                           FutureWarning)\n\u001b[1;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m    804\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m         y = check_array(y, accept_sparse='csr', force_all_finite=True,\n\u001b[0;32m--> 806\u001b[0;31m                         ensure_2d=False, dtype=None)\n\u001b[0m\u001b[1;32m    807\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m                           FutureWarning)\n\u001b[1;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    644\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m             _assert_all_finite(array,\n\u001b[0;32m--> 646\u001b[0;31m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m     98\u001b[0m                     \u001b[0mmsg_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                     (type_err,\n\u001b[0;32m--> 100\u001b[0;31m                      msg_dtype if msg_dtype is not None else X.dtype)\n\u001b[0m\u001b[1;32m    101\u001b[0m             )\n\u001b[1;32m    102\u001b[0m     \u001b[0;31m# for object dtype data, we only check for NaNs (GH-13254)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "# initial sampling = 10, BO for 50 iterations\n",
    "xgb_bo.maximize(n_iter=50, init_points=10, acq='ei')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eta': 0.1493620343918557, 'gamma': 0.2800607413396208, 'max_depth': 6.454884073943703, 'n_estimators': 96.79557626993903}\n"
     ]
    }
   ],
   "source": [
    "# returns best performing params dict\n",
    "params = xgb_bo.max['params']\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['max_depth']= int(params['max_depth'])\n",
    "params['n_estimators']= int(params['n_estimators'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eta': 0.1493620343918557,\n",
       " 'gamma': 0.2800607413396208,\n",
       " 'max_depth': 6,\n",
       " 'n_estimators': 96}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_reg = xgb.XGBRegressor(**params).fit(np.array(X_feature), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9815373935695506e-07"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(xg_reg.predict(np.array(X_feature)) - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
